---
phase: 23-canvas-matrix-rain
plan: 04
type: execute
wave: 3
depends_on:
  - 23-02
  - 23-03
files_modified: []
autonomous: false
requirements:
  - ANIM-03

must_haves:
  truths:
    - "lhci autorun reports performance score >= 0.90 on all five portfolio URLs"
    - "The Lighthouse CI gate passes — Phase 24 may now proceed"
  artifacts:
    - path: "apps/web/lighthouserc.json"
      provides: "Lighthouse CI config already in place from Plan 02"
      contains: "minScore.*0.9"
  key_links:
    - from: "apps/web/lighthouserc.json"
      to: "lhci autorun"
      via: "npx lhci autorun after npm run build"
      pattern: "autorun"
---

<objective>
Run the full Lighthouse CI gate against a production build to confirm performance scores >= 0.90 on all five portfolio URLs.

Purpose: ANIM-03 — this is the hard gate before Phase 24. Canvas must not degrade Lighthouse performance below 0.90. The production build is required (not dev server) for valid scores.
Output: Confirmed lhci autorun passing on all five URLs, or documented remediation if scores are below threshold.
</objective>

<execution_context>
@/home/doctor/.claude/get-shit-done/workflows/execute-plan.md
@/home/doctor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/23-canvas-matrix-rain/23-02-SUMMARY.md
@.planning/phases/23-canvas-matrix-rain/23-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build production bundle for apps/web</name>
  <files>apps/web/.next</files>
  <action>
    Check if a Docker container is running apps/web in production mode on port 3000:
    ```bash
    docker ps 2>/dev/null | grep -E "3000|apps/web|portfolio" || echo "no container"
    ```

    If a production container is already running, stop it first to avoid port conflicts when lhci starts its own server:
    ```bash
    docker stop <container_id>
    ```

    Then build the production bundle from the monorepo root:
    ```bash
    npm run build --workspace apps/web
    ```

    This compiles the Next.js app including the new MatrixRainCanvas component. A successful build proves the canvas integration has no build errors.

    IMPORTANT: Do NOT run `lhci autorun` against `next dev`. Lighthouse scores on the dev server will be 40-60 and meaningless. Only run against a production build.
  </action>
  <verify>
    Build output ends with a Next.js success message. No TypeScript errors. No webpack errors. Exit code 0.
  </verify>
  <done>
    Production build succeeds. `.next/` directory is populated with optimized output ready for `npm run start`.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Run lhci autorun and verify performance gate</name>
  <action>
    This is the ANIM-03 hard gate. Run from `/home/doctor/fernandomillan/apps/web`:
    ```bash
    npx lhci autorun
    ```

    lhci will start the production server, run Lighthouse 3 times on each of the five URLs, average the scores, assert performance >= 0.90, and print report URLs.

    If a score is below 0.90 on any URL, apply remediations in order:
    1. Reduce FPS_TARGET to 20 in matrix-rain-canvas.tsx (FRAME_INTERVAL = 1000 / 20)
    2. Add `will-change: transform` to canvas style prop for GPU layer promotion
    3. Increase FONT_SIZE to 18 (fewer columns, fewer fillText calls per frame)
    After any remediation: rebuild and re-run lhci autorun.
  </action>
  <verify>
    `npx lhci autorun` exits with code 0 and output includes "All assertions passed". Each of the five URLs shows performance score >= 0.90 in the summary table.
  </verify>
  <done>
    All five URLs score >= 0.90. ANIM-03 is satisfied. Phase 24 may proceed.
  </done>
  <what-built>
    Plan 01 created the MatrixRainCanvas component and integrated it into the hero section.
    Plan 02 installed @lhci/cli and fixed lighthouserc.json to test the five required URLs.
    Plan 03 updated Playwright snapshots.
    Task 1 of this plan built a production bundle.

    Now lhci autorun must be executed to confirm ANIM-03 is met.
  </what-built>
  <how-to-verify>
    Run from `/home/doctor/fernandomillan/apps/web`:
    ```bash
    npx lhci autorun
    ```

    Expected: "All assertions passed". Each URL shows performance >= 0.90.

    If below 0.90 on any URL:
    - Open the LHCI report URL printed in the output
    - Check "Opportunities" section for the regression cause
    - Apply remediations listed in the action above
    - Rebuild and re-run

    Report the exact scores for all five URLs.
  </how-to-verify>
  <resume-signal>
    Type "passed" if all five URLs score >= 0.90, or paste the score table and describe which URLs failed and what Lighthouse flagged as the cause.
  </resume-signal>
</task>

</tasks>

<verification>
Phase 23 complete when:
1. `lhci autorun` exits with code 0 and "All assertions passed"
2. All five URLs score >= 0.90 on the performance category
3. The LHCI report URLs are captured in the SUMMARY for the record
</verification>

<success_criteria>
- `lhci autorun` exits code 0 — "All assertions passed"
- Performance score >= 0.90 on `/`, `/projects`, `/projects/teamflow`, `/projects/devcollab`, `/contact`
- This is a HARD GATE — Phase 24 does not start until this passes per STATE.md constraints
</success_criteria>

<output>
After completion, create `.planning/phases/23-canvas-matrix-rain/23-04-SUMMARY.md` including the exact Lighthouse scores for all five URLs and the LHCI report links.
</output>
